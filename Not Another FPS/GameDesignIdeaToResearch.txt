-Footsteps: have a detailed rig, skin, and animation of a biped (or multiped) character. Attach colliders on each foot, attach an audio player on each foot. On collision (not just contact), query the contact surface what sound to play. Create a player for every foot and play the sound independently. Goal: create the correct number of stepping sound of a biped (human), octoped (spider), or any other creatures. Have the sound vary according to the animation, and the material property of the impact surface. Improvement: procedurally generate the footsteps. Requirements: find out how, if possible, does Unity differentiate a contact and an impact. Then find 2 animation rigs, and the animation, of a human walking and running. Then progress on to some crawling octopeds.

-Unity Footsteps: have a detailed rig, skin, and animation of a biped (or multiped) character. Attach colliders on each foot, attach an audio player on each foot. On collision (not just contact), query the contact surface what sound to play. Create a player for every foot and play the sound independently. Goal: create the correct number of stepping sound of a biped (human), octoped (spider), or any other creatures. Have the sound vary according to the animation, and the material property of the impact surface. Improvement: procedurally generate the footsteps.
Results: The footsteps are very convincing when the sound generation is tied up to the actual 3rd person animation. Each sound fx just have to be very very short. I am talking about in the range of 50ms, but I have to try it out.
I just have to place the collider on the Vanguard's toes as Trigger colliders. Toes collision time makes a more convincing sound generation time when looking at the thrid person animation. 

-RTS military squad unit shooting skill AI:
The AI shooting experience level increases with #of kills, #of survived missions, To shoot and damage a target, I consider 2 approaches:
1. The AI agent always aim at the fatal hit area of the target. The shooting quality is then defined by a gaussian distribution on an aiming circle. The radius of the circle shrinks as the AI shooting experience increases. The hit location of the target is the raycast between one dot from the gaussian distribution result, and the shooter's center. This approach is costly, but allows for the target unit to maneuver in ways that minimizes critical area exposure.
This is expensive compared to (2), but allows the user to tactically maneuver individual squads under fire. Game experience may be more dynamic, because a well-entrenched mediocre vanguard unit should be able to stand up against superior AI in a delaying retreat.
2. The AI agent inflicts damage percentage proportional to the experience level. This is the classic dice RPG method. Cheap because the target is a black box. However I suspect that the game dynamics can be predictable, especially when a bunch of experienced AI fights numerically superior mediocre AI.

-Unity user-controllable character movement: I like Penny's approach of attaching a ray caster at the tip of the weapon, with a spot light to project the cross hair. I just don't like the coommon camera tilting approach taken by many FPS controller. I decided to manipulate the character's spine, now that I have figured out how to do this in Unity.
Result: The spine manipulation is not good for the FPS camera, because the x rotation axis end up tilting the camera at an angle. Plus there is a bunch of cam stabilization problem also that I would like to tackle.

-Unity user-controllable character - terrain interaction. The topmost parent node still need a rigidbody and a collider. All childred ragdoll joints must have their colliders set to IsTrigger

-Head Stabilization: See https://matthew-isidore.ovh/full-body-fps-controller-part-1-base-character-controller/, which also teaches me to use the Animator Blend Tree, and the accompanying youtube video link to Chris Roberts' Star Citizen video feed. In case I forget who Chris Roberts is, that's the Wing Commander guy. Yeah,, THE GUY who produced Wing Commander series. I got the far focal point system implemented. Surprisingly, the initial concept is very simple. The character, Vanguard, has a very stable pivot point axis. So I added a faraway sphere, at the head level, as the child of the Vanguard. Then I constrain the camera to look at the sphere. This is quite good at eliminating the camera shakes, and also align the camera axis to the world axis. Next I need to work this with bending. The video feed also talks about head IK, which I will learn from the website.
I wonder why constraints cannot be used here. Maybe the solution is not portable across many animations.

I stumbled on a nice solution to stabilize the head and have a nice to see gun view. The head stabilization above still apply. I tweaked the head lookat position so the head and the animation rootAnimation move in the same direction.
The trick part is the gun. I don't like the arm positions of the original animation because I think the gun is just too high. So I decided to manually place the gun at around waist level. The hand position will be derived from IK computations. I first parent the gun to the head. Nice that the head and the gun moves in the same direction, but the IK won't work. The hands will not resolve when the head and the gun start moving around. IK only works when I parent the gun with the same parent as the right and left shoulders. The head lookat is not a constraint anymore. It is now an IK-resolved lookAt. The camera is still constrained to the lookat point. No doing so does not fix the original head stabilization problem, despite the camera being the child of the head. Weird.

Constraints and IK do not work. The gun initially was parent constrained to the spine2. The hand placements and the hands IK will not resolve at all. It seems that the Engine resolves constraint-based transformations after resolving IK. 

Once I move the new Vanguard model to the Viking village, turning on the rigidbody gravity, with mass=1 levitates the model. Weird. Must be some setting in the village I am not aware of.

The body collider does not cover the legs. I need to change the jump state to now look for ground contact using raycasthit. Supposedly this is the way to allow the feet to IK correctly when climbing stairs

There seems to be at least 2 ways to make the character obey the terrain: Use a combination of non-trigger collider and gravity-activated non-kinematic rigidbody, or turning the character into a nav agent with a character controller component.

Animation Blending: See FPS Controller Tutorial->AnimationBlend scene. I am following the tutorial from https://diegogiacomelli.com.br/unity-avatar-mask-and-animation-layers/. I created an Animation Controller called AnimationBlend. This Animator has 2 layers: the base layer, and the upper body layer. The key to mixing the animation in the same currently playing animation state is the Avatar Mask. You can create an Avatar Mask, and use this to mask off animation in the body portion. Upper body layer uses an Upper Body avatar mask, which mask off lower body parts. i suspect that the animations must be rigged correctly. If using a humanoid animation, then the animation must be rigged with the correct avatar. I use the same avatar for the Avartar Mask transition. Not sure what this does yet. The pain in the butt is making the Any State transition to the next looping animation state without "Has Exit Time". In Settings, I have to uncheck "Can Transition To Self". I never mess around with this setting until now, and all of my looping animations seems working correctly. Seems like a transition from Any State to a looping animation state, with Has Exit Time unchecked, has the tendency to transition to self, which seems to jam the animation loop.

Zombie AI: If I want to use colliders as detecors, they have to be trigger colliders.